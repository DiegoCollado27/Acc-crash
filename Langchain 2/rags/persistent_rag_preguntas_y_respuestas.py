# -*- coding: utf-8 -*-
"""persistent-rag-preguntas_y_respuestas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UN47cZSG6HQm_iWRUiAAMLX7r4thL-a0

# Preguntas y Respuestas con RAG Langchain

Usando Chroma DB and LangChain para contestar preguntas de un texto, con una base de datos local. Guardamos los embaddings, y los usamos después.
"""

#pip install chromadb langchain langchain-openai

from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings

from google.colab import userdata
openai_token = userdata.get('OPENAI_TOKEN')

"""## Cargar y procesar documentos.
aqui cargamos los documentos sobre los que vamosa  aplicar el Q&A

Los dividimos en trozos (chunks). ASi buscamos los más relevantes y los pasamos al LLM.
"""

# Load and process the text
loader = TextLoader('state_of_the_union.txt')
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

"""## Inicializamos PeristedChromaDB

Creamos los embeddings. Y los guardamos en la db
"""

# Directorio donde se guardará la db
persist_directory = 'db'

engine = "gpt-4"
embedding = OpenAIEmbeddings(api_key=openai_token, model="text-embedding-3-large")

vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)

"""## Persist the Database
In a notebook, we should call `persist()` to ensure the embeddings are written to disk.
This isn't necessary in a script - the database will be automatically persisted when the client object is destroyed.
"""

vectordb.persist()
vectordb = None

"""## Cargamos la db del disco
`persist_directory` es el directorio donde se va a guardar la db y `embedding_function` tiene que ser la misma que cunado instanciamos a db. Se encargara de las respuestas.
"""

# Cargamos la base de datos persistente del disco y usamos un langchain retrieval
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
# Deprecated since version 0.1.17: Use create_retrieval_chain instead.
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(api_key=openai_token),
    retriever=vectordb.as_retriever(
        search_kwargs={"k": 4})
    )

"""## Preguntas y respuestas

Con langchainy RAG
"""

query = "What was the growth of manufacturing jobs in US last year"
result = qa.invoke(query)
result.get("result")

"""## Limpieza

Cuando hayas terminado con la base de datos, puedes eliminarla del disco. Puedes eliminar la colección específica con la que está trabajando (si tiene varias) o eliminar toda la base de datos destruyendo el directorio de persistencia.
"""

# limpiamos la colección
vectordb.delete_collection()
vectordb.persist()

# nos cargamos el directorio de persistencia
#rm -rf db/