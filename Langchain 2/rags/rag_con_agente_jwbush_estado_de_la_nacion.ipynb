{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain wikipedia langchain-community tavily-python langchainhub langchain-openai openai chromadb"
      ],
      "metadata": {
        "id": "PxNlsnAsumbc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbx3soAUuhJ0"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain langchain_openai langchainhub chromadb\n",
        "\n",
        "from google.colab import userdata\n",
        "secret = userdata.get('OPENAI_TOKEN')\n",
        "\n",
        "# Load docs\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://georgewbush-whitehouse.archives.gov/news/releases/2003/01/20030128-19.es.html\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "# Store splits\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(openai_api_key=secret))\n",
        "\n",
        "# RAG prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=secret)\n",
        "\n",
        "# RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "question = \"¿Qué conclusiones se obtienen sobre Iraq?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n"
      ]
    }
  ]
}