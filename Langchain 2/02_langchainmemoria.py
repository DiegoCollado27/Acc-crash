# -*- coding: utf-8 -*-
"""02 - LangchainMemoria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Dx3gBcevnUkkOX2J9Bpz341G9q5_vW0
"""

# Si no tenemos el módulo instalado
!pip install langchain-openai
!pip install langchain
!pip install tiktoken
!pip install cohere

"""## Memoria - ConversationBufferMemory

"""

from google.colab import userdata


secret_string = userdata.get('OPENAI_TOKEN')
# Configurar el motor de OpenAI
engine = "gpt-4"

from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(temperature=0.0, model=engine, openai_api_key=secret_string)
memory = ConversationBufferMemory()

conversation = ConversationChain(
    llm=llm,
    memory = memory,
    verbose=False # Si ponemos verbose=True devuelve datos de la plantilla
)

conversation.predict(input="Hola, mi nombre es Carlos")

conversation.predict(input="¿Cuál es la capital de Honduras?")

conversation.predict(input="¿Cuál es mi nombre?")

print(memory.buffer)

"""## ConversationBufferWindowMemory"""

from langchain.memory import ConversationBufferWindowMemory

llm = ChatOpenAI(temperature=0.0, model=engine, openai_api_key=secret_string)
## USamos conversationBufferWindowMemory para almacenar el contexto de la conversación

memory = ConversationBufferWindowMemory(k=4) # Numero de interacciones en AI y usuario

conversation = ConversationChain(
    llm=llm,
    memory = memory,
    verbose=False # Si usas True podrás ver el prompt real que se está pasando al modelo

)

conversation.predict(input="Hola, mi nombre es Carlos")

conversation.predict(input="¿Cuál es la capital de Honduras?")

conversation.predict(input="¿Cuál es mi nombre") # No lo recordará ya que k=1. Cambia k=2 para ver que funciona

## ConversationSummaryMemory

from langchain.memory import ConversationSummaryBufferMemory

# creamos un texto
schedule = "Hay una reunión a las 8 a. m. con tu equipo de producto. \
Necesitarás tener preparada tu presentación de PowerPoint. \
De 9 a. m. a 12 p. m. tienes tiempo para trabajar en tu proyecto LangChain, \
que avanzará rápidamente porque LangChain es una herramienta muy potente.\
Al mediodía, almuerzo en el restaurante italiano con un cliente que viene conduciendo\
desde más de una hora de distancia para reunirse contigo y entender lo último en IA.\
Asegúrate de traer tu portátil para mostrar la última demo de LLM."

memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
memory.save_context({"input": "Hola"}, {"output": "¿Qué hay?"})
memory.save_context({"input": "Hoy es el último día para reservar plaza en el congreso"},
                    {"output": "Guay"})
memory.save_context({"input": "Qué hay en la agenda hoy?"},
                    {"output": f"{schedule}"})

memory.load_memory_variables({})

conversation = ConversationChain(
    llm=llm,
    memory = memory,
    verbose=True
)

#conversation.predict(input="Qué hay en la agenda para hoy?")
#conversation.predict(input="Algún evento importante que deba recordar?")
conversation.predict(input="Que plan hay para hoy?")