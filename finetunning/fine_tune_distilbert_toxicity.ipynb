{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Usando CPU\")"
      ],
      "metadata": {
        "id": "zMf11LWhPq5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8WrWzhw4GDm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "data_dir = 'data/jigsaw_toxicity_comment'\n",
        "if not os.path.exists(f'{data_dir}/toxic_train_processed.csv'):\n",
        "    # Cargar el dataset de Toxic Comment Classification Challenge\n",
        "    import pandas as pd\n",
        "    data_dir = 'data/jigsaw_toxicity_comment'\n",
        "    # Cargar los archivos CSV\n",
        "    train_df = pd.read_csv(f'{data_dir}/toxic_train.csv')\n",
        "\n",
        "    # Dividir el dataset de entrenamiento en entrenamiento y validación\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Guardar los archivos procesados\n",
        "    train_df.to_csv(f'{data_dir}/toxic_train_processed.csv', index=False)\n",
        "    val_df.to_csv(f'{data_dir}/toxic_val_processed.csv', index=False)\n",
        "\n",
        "\n",
        "    # Cargar el dataset desde el directorio local\n",
        "dataset = load_dataset('csv', data_files={'train': f'{data_dir}/toxic_train_processed.csv', 'validation': f'{data_dir}/toxic_val_processed.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "EyXa3O2a4V9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usamos el tokenizador de Distibert para nuestra muestra"
      ],
      "metadata": {
        "id": "l-Qm770iIK3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "import transformers\n",
        "import accelerate\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Accelerate version:\", accelerate.__version__)\n",
        "# Inicializamosel tokenizer de distilBERT\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# definimos las etiquetas\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Funcion de tokenización y perparacion de etiquetas\n",
        "def tokenize_and_encode(examples):\n",
        "    encodings = tokenizer(examples['comment_text'], padding='max_length', truncation=True)\n",
        "    labels_matrix = np.zeros((len(examples['comment_text']), len(labels)))\n",
        "    for i, label in enumerate(labels):\n",
        "        labels_matrix[:, i] = examples[label]\n",
        "    encodings['labels'] = labels_matrix.tolist()\n",
        "    return encodings\n",
        "\n",
        "# Aplicamos la tokenización y etiquetas procesada\n",
        "tokenized_datasets = dataset.map(tokenize_and_encode, batched=True, remove_columns=['comment_text'] + labels)\n",
        "#tokenized_datasets.set_format(\"torch\")\n",
        "train_dataset = tokenized_datasets['train']\n",
        "val_dataset = tokenized_datasets['validation']\n",
        "\n"
      ],
      "metadata": {
        "id": "WaUFFYVI4VA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos el modelo, la clase para entrenar y la clase de configuración\n",
        "### En este caso lo preparamos para multietiqueta"
      ],
      "metadata": {
        "id": "bj9if1iDISiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Cargar el modelo preentrenado con el número adecuado de etiquetas\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))\n"
      ],
      "metadata": {
        "id": "c0O20tFcIHio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = (pred.predictions > 0.5).astype(int)  # Convertir predicciones a binario usando un umbral\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "4mKlyL6NOkmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si queremos continuar desde un chepoint\n",
        "model = DistilBertForSequenceClassification.from_pretrained(f\"{data_dir}/distilbert-finetuned-toxic\")"
      ],
      "metadata": {
        "id": "4UO7L-bEouLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eKAb3zjDIr25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "lbY1Me-cO6HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamos"
      ],
      "metadata": {
        "id": "fvgzsMHyKREY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "Abkj36ptKUFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "BLjx8aWvUGvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{data_dir}/distilbert-finetuned-toxic\")\n",
        "tokenizer.save_pretrained(f\"{data_dir}/distilbert-finetuned-toxic\")"
      ],
      "metadata": {
        "id": "7cqvDHdqUJji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_labels_test = pd.read_csv(f\"{data_dir}/test_labels.csv\")\n",
        "df_datos_test = pd.read_csv(f\"{data_dir}/toxic_test.csv\")\n",
        "df_conjunto = df_datos_test.merge(df_labels_test, how='left', on='id')\n",
        "df_conjunto_filtrado = df_conjunto[~(df_conjunto[labels] == -1).all(axis=1)]\n",
        "df_conjunto_filtrado = df_conjunto_filtrado.drop(['id'], axis=1)\n",
        "df_conjunto_filtrado = df_conjunto_filtrado.reset_index(drop=True)\n",
        "df_conjunto_filtrado.head()"
      ],
      "metadata": {
        "id": "fCLbpgpxjUFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "# Verificando si hay una GPU disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "correctos = 0\n",
        "aceptables = 0\n",
        "incorrectos = 0\n",
        "# Movemos el modelo al dispositivo\n",
        "model = model.to(device)\n",
        "for _ in range(1000):\n",
        "    random_index = random.randint(0, df_conjunto_filtrado.shape[0] - 1)\n",
        "\n",
        "    fila = df_conjunto_filtrado.iloc[random_index]\n",
        "    # Texto de ejemplo para probar el modelo\n",
        "    text = fila['comment_text']\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    #movemos los tensores de entrada al dispositivo\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Deshabilitar el cálculo de gradientes para ahorrar memoria y mejorar la velocidad\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # kas salidas contienen los logits\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # obtenemos las probabilidades con sigmoid a partir de los logits\n",
        "    probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    # sacamos las etiquetas predichas (usando un umbral, por ejemplo 0.5)\n",
        "    predicted_labels = probabilities > 0.5\n",
        "\n",
        "    # asociamso etiquetas predichas con etiquetas reales\n",
        "    prediccion_valores = [labels[i] for i in range(len(labels)) if predicted_labels[i]]\n",
        "    valores_esperados = [labels[i] for i in range(len(labels)) if fila[labels[i]] == 1]\n",
        "    set_prediccion = set(prediccion_valores)\n",
        "    set_esperados = set(valores_esperados)\n",
        "\n",
        "    if set_esperados == set_prediccion:\n",
        "        print(f\"✅ Correcto {valores_esperados}\")\n",
        "        correctos += 1\n",
        "    elif set_esperados.issubset(set_prediccion) and set_esperados and not set_prediccion.issubset(set_esperados):\n",
        "        aceptables += 1\n",
        "        print(f\"😊 Esperado {valores_esperados} -- Prediccion {prediccion_valores}\")\n",
        "    else:\n",
        "        incorrectos += 1\n",
        "        print(f\"❌ Esperado {valores_esperados} -- Prediccion {prediccion_valores}\", f\"Texto: {fila['comment_text']}\")\n",
        "    #print(f\"Probabilities: {probabilities}\")\n",
        "    #print(f\"Predicted labels: {predicted_labels}\")\n",
        "    #print(f\"Predicted label names: {predicted_labels_names}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZBUAGtSgb2XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Correctos\", correctos)\n",
        "print(\"Aceptables\", aceptables)\n",
        "print(\"Incorrectos\", incorrectos)\n",
        "print(\"Relevancia Aprox\", round((correctos + aceptables) / (correctos + aceptables + incorrectos) * 100, 2), \"%\")\n",
        "print(\"Aciertos 100% Aprox\", round((correctos) / (correctos + aceptables + incorrectos) * 100, 2), \"%\")"
      ],
      "metadata": {
        "id": "f2crt8GN5V1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}